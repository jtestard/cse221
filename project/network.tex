The setup for our networking experiments consisted of our local machine
connected to a router and a remote machine connected to the same router over a
100Mbps Ethernet network. See the machine description section for a description
of the remote machine.

The source code for the programs that implement our experiments make use of the
linux sockets api. The relevent data structures are initialized for TCP connections.
We use the socket() call to request a socket file descriptor,
connect() to initiate a TCP connection, send() to send data, and recv() to receive
 data. The close() call terminates the TCP connection.

 The remote machine possesses an AMD(R) Phenom(TM) II X4 925 processor with 4 cores.
 \begin{itemize}
     \item{each core has a clock frequency of 2.80 GHz}
     \item{6GB of main memory}
     \item{Running 32-bit Cygwin Linux environment on a Windows 8 host}
     \item{100Mbps ethernet card}
 \end{itemize}

\subsection{Round Trip Time (RTT)}

\subsubsection{Estimation}

Our estimation of RTT for TCP is based upon an empirically derived lower bound.
The lower bound was determined by using the ping command to observe RTT for
ICMP packets sending and receiving 56 bytes of data to a remote host and
loopback. The overhead of TCP is greater than that of ICMP due to it being a
connected protocol that requires handshaking to establish a connection,
additional handshaking to close the connection, and a larger packet size.
Therefore, the RTT using TCP should be no faster than a ping to the same host. 

We observed that the average RTT for 1000 pings to the loopback were 0.033 and
to the remote machine 0.955ms. Transmitting 56 bytes of data via TCP requires
3.5 additional round trips because of connection overhead for the initial 3-way
handshake and 4-way termination which consists of 7 packets that each travel
one way.  We estimate the minimum RTT to loopback for an equivalent amount of
data (56 bytes) via TCP to be $$4.5 * 0.033\text{ms} = 0.145\text{ms}.$$  

The difference between the the ping time for loopback and to the remote machine
represent the overhead of packets crossing the physical network. Therefore we
estimate that the additional RTT for a single packet to the remote machine is
$0.955\text{ms} - 0.033\text{ms} = 0.922\text{ms}$. Clearly, the overhead of
crossing the physical network dominates the time to evaluate and send a packet
at its destination and source machine. It follows from our estimation of the
RTT to loopback that RTT to the remote machine for 56 bytes of data via TCP is
calculated as $$4.5 * 0.033\text{ms} + 4.5 * 0.922\text{ms} = 4.294\text{ms}.$$

\subsubsection{Methodology}

The program which runs on the local machine sets up a TCP connection and sends
a message to the remote machine. It waits for the message to return and then
tears down the connection. Each timing iteration begins immediately before
setup of the TCP connection and ends immediately after the connection is
closed. The program initializes a character array of 56 elements (i.e. 56
bytes) in order to perform an accurate comparison with ping which sends 56
bytes of payload per packet.  We use the sockets library to obtain a file
descriptor to a socket and then connect to the remote machine. We send to
socket 7 in order to be consistent with RFC 862 \cite{rfc862} which sets the
specification for an echo service. Once a connection is successful we send the
56 byte message and then perform a blocking read which will wait for the
message to return from the remote machine. After a message is recieved we close
the TCP connection and stop timing. If the bytes read does not equal the bytes
sent, the result is not recorded due to packets being dropped and the iteration
is repeated until successful.

In order to implement a simple echo service on the remote machine, we used the
ncat program with the following command: 
\begin{verbatim}
ncat -l 7 --keep-open -c 'cat'
\end{verbatim}
The arguments to the command instruct the program to listen on port 7 for
incoming tcp connections and echo the data by executing the cat program on the
incoming data. The keep open parameter instructs ncat to continue listening for
new connections once a remote connection closes.

\subsubsection{Results}

We used the timing methodology described earlier in this paper that reports the
number of cycles elapsed between the start and end. We convert the cycle count
to milliseconds as described in timing overhead section and present our results
in table~\ref{table:rtt-results}.

\begin{table*}[b]
\begin{tabular}{|c|c|c|c|}
\hline
RTT Local (cycles) & RTT Local (ms) & RTT Remote (cycles) & RTT Remote (ms) \\ \hline
3850916            & 1.572          & 30172312            & 12.315          \\ \hline
\end{tabular}
\caption{RTT experimental results}
\label{table:rtt-results}
\end{table*}

\subsubsection{Analysis and Discussion}

Our estimates for the remote case were off by a factor of 3 and an order of
magnitude in the case of loopback. We realize that in our estimates we did not
account for the fact that TCP would require an work done at an additonal layer
of the TCP/IP stack versus the ICMP packet. This is due to ICMP being a network 
layer protocol, the same as IP which encapsulates the ICMP packet, while TCP is 
a transport layer protocol. The evaluation/formation of the TCP packet at the
transport layer adds additional overhead that is missing in our estimate, but 
represented in the experimental results. This overhead would be present in the 
local and remote machine. The router that connects the two machines operates 
at the network layer and would not contribute additional overhead versus ICMP 
because it does not read TCP header information.

Comparing the local and remote results reveals that the overhead of transmitting
over a network dominates that of the OS overhead by an order of magnitude 
even for machines connected to the same LAN.

The network is rated at a maximum of 100Mbps. Our results show that we achieve in 
the local case: $$\frac{(56*8)}{0.00157} * 10^{-6} = 0.296\text{Mbps}$$
and in the remote case: $$\frac{(56*8)}{0.0123} * 10^{-6} = 0.0364\text{Mbps}.$$
This is far below the maximum performance that the hardware is capable of because of
 the software overheads that are required for reading and creating packet header information.
Ultimately, it is the size of our payload that contributes most to the discrepency 
in performance. The maximum transmission unit (MTU) for TCP is 1460 bytes, 
however we are sending data in 56 byte chunks. The next experiment in peak bandwidth 
will provide more reasonable results regarding the difference between ideal 
hardware performance and real world performance over TCP.

\subsection{Peak Bandwidth}

\subsubsection{Estimation}

We use the data collected in the previous experiment to assist us in
deriving an estimate for peak bandwidth over TCP to a remote and local
connection. We determined in the previous experiment that in order to 
gain maximum throughput with TCP we would need each payload to be 
equivalent to the MTU. The difference in throughput should increase 
by a factor of $\frac{1460}{56} = 26.07$. We believe the OS overhead at the transport 
layer of reading and forming packets will also increase due to the larger payload, but 
this should have a negligable effect on our estimate due to the message buffer being 
initialized to the size of MTU in advance. With this information we estimate that 
peak bandwidth for the local case will be: $$0.297\text{Mbps} * 26.07 = 7.74\text{Mbps}$$
and for the remote case: $$0.0364\text{Mbps} * 26.07 = 0.949\text{Mbps}$$.

\subsubsection{Methodology}

We create two programs for this experiment: a writer program that connects via
tcp to a remote server and sends data to the server; a reader program which
implements a server that is listening for incoming tcp connections, reads data,
and performs timing.

The writer program initializes a character array that is 5MB in size. 
A socket is initialized and the socket send buffer is set to 1MB in size \cite{lmbench}. 
The writer connects to the remote server that is running the reader program and begins timing. 
The 5MB message is sent. If the entire message is sent successfully timing is stopped and 
the socket is closed, otherwise we discard this iteration and start over. 
We iterate this process 100 times and select the median result.

The reader program implements a server that listens for a connection and reads in data.
It begins by initializing a socket and setting its receive buffer to 1MB in size \cite{lmbench}.
After binding to the socket, it blocks while listening for an incoming connection. Timing begins
once an incoming connection is accepted. It then begins to receive the message into an input buffer.
Once the entire message has been recieved, timing is stopped and the connection from the client is
closed. At this point the server is ready for a new incoming connection.

\subsubsection{Results}
The results from this experiment are tabulated below. The reader program outputs the result in bytes per cycle (Bpc). This value is converted to megabits per second (MBps) using the clock period established in the timing section of this report. We present our results in table~\ref{table:peak-results}.

\begin{table*}[b]
\begin{tabular}{|c|c|c|c|}
\hline
Peak BW Local (Bpc) & Peak BW Local (Mbps) & Peak BW Remote (Bpc) & Peak BW Remote (MBps) \\ \hline
0.1381            & 2707          & 0.0008            & 16          \\ \hline
\end{tabular}
\caption{Peak bandwidth experimental results}
\label{table:peak-results}
\end{table*}

\subsubsection{Analysis and Discussion}
According to our experimental results, our estimates vastly underestimated the peak bandwidth of TCP.
In this experiment the initialization handshake and termination handshake of TCP are left out of the timing loop. This leads us to conclude that once a connection is established data transfer over TCP 
proceeds with a much lower overhead. Since we did not account for this, we under estimated the peak
bandwidth.

The difference between peak bandwith for the local and remote cases illustrates that the latency of 
transmitting over the network dominates the OS overhead when transferring data over TCP. The local 
peak bandwith exceeds the 100Mpbs capability of our ethernet network because the data never reaches
the ethernet portion i.e. the writer program send the data down the TCP/IP stack and the reader 
program which is running in a separate process receives the data and sends it up the TCP/IP stack via
 a software pipe. 

The remote peak bandwith results show that the we are achieving less than 20\% of the hardwares 
maximum performance. Since the local peak bandwidth experiment indicates that software is not the 
bottleneck we must conclude that the bulk of the overhead resides in hardware at the link layer and
physical ethernet connection. 

\subsection{Connection Setup and Teardown}

\subsubsection{Estimation}
We use our experimental results from RTT to assist us in estimating the connection teardown and setup
overhead. We determine the estimate for sending a single packet by dividing the result from RTT by the number of packets for the 3-way handshake connection, 4-way handshake termination, and 2-ways for roundtrip of data. This should total 9 packets. It follows that, for the local case the time to send and receive a single packet is approximately $\frac{1.572\text{ms}}{9} = 0.175\text{ms}$ and for the remote case $\frac{12.315\text{ms}}{9} = 1.368\text{ms}$. Our estimates are show in table~\ref{table:connection-estimates} and were calculated by multiplying the number of packets required to perform connection (3) or teardown (4) by the time to send and receive a single packet.

\begin{table*}[b]
\begin{tabular}{|c|c|c|c|}
\hline
Setup Local(ms) & Setup Remote (ms) & Teardown Local (ms) & Teardown Remote (ms)\\ \hline
0.525            & 0.700          & 4.104            & 5.472          \\ \hline
\end{tabular}
\caption{Estimates for TCP connection setup and teardown overhead}
\label{table:connection-estimates}
\end{table*}

\subsubsection{Methodology}
The programs for determining are identical except for where the timing begins and ends. The programs 
request a socket, connect to the socket, and then close the socket. For measuring connection time we
begin timing before the request for a socket and end after a connection.  For measuring teardown time
 we begin timing after the connection is established and after the connection is closed. The 
 procedure is repeated for 1000 iterations and the median value is reported as the result.

 On the remote end we utilize the ncat program to setup a simple server that binds to port 2000 and listens for incoming connections with the following command:
 \begin{verbatim}
 ncat -l 2000 --keep-open
 \end{verbatim}
\subsubsection{Results}
Due to our timing methodology, both programs report results in terms of elapsed cycles. We convert this to milliseconds by dividing by the clock frequency reported in the machine description of this report.
The results are show in table~\ref{table:connection-results}.

\begin{table*}[b]
\begin{tabular}{|c|c|c|c|}
\hline
Setup Local(ms) & Setup Remote (ms) & Teardown Local (ms) & Teardown Remote (ms)\\ \hline
0.149            & 0.983          & 0.315            & 0.014         \\ \hline
\end{tabular}
\caption{Experimental results for TCP connection setup and teardown overhead}
\label{table:connection-results}
\end{table*}

\subsubsection{Analysis and Discussion}
In general, we overestimated the overhead of both setup and teardown. An interesting result is 
that the teardown time is much longer in the local case than in the remote case. We assumed that a 
call to close() would send the packets required for the 4-way termination handshake. When we looked 
into this further we learned that close() deallocates the socket file descriptor and any file 
descriptors associated with pipes or FIFO. The local case taking much longer to close may be due to 
additional pipes that facilitate the connection to the ncat server running in another process.
This also explains why our initial estimate that teardown time would exceed setup time is invalid.
The additional time to setup a TCP connection in the local case is likely due to the extra overhead 
of inter process communication with the ncat server.

