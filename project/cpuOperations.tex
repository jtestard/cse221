\subsection{Measurement Overhead}

In order to accurately measure execution time we implemented a timing
methodology that reads the hardware time-stamp counter before and after a
procedure (by procedure here we mean lines of C or assembly code we wish to
profile, not necessarily a procedure call).  The time-stemp is read immediately
before the procedure is called and again after the procedure exits. The two
output values correspond to the start and end times of the procedure being
executed. The initial time is subtracted from the final time to calculate the
number of cycles that have elapsed.

The implementation for reading the time-stamp makes use of the assembly
instructions CPUID, RDTSC, and RDTSCP. CPUID and RDTSCP are used to force
serialization of instructions so that instructions which are not part of the
code being instrumented do not pollute our measurements due to out-of-order
execution.\cite{intel}

We run the timing harness and procedure being timed in a loop of 10,000
iterations. The execution time is collected on each iteration of the loop. We
use the median execution time over all iterations as the final result in order
to reduce the influence that extreme values caused by context switches,
interrupts and other exceptional events have on our experiment. 

\subsubsection{Measuring time overhead} 

We estimate that the hardware overhead of reading the time-stamp registers and
calculating the elapsed time is 2 cycles. Since the time-stamp is implemented in
registers it would require 1 cycle to read the current time. 

We implemented the reading of the start and end time with inline assembly. The
code to read the start time and the code to read the end time each consists of
four assembly instructions. This includes the calls to CPUID which act as a
fence to force sequential execution of code.  Since the time-stamp register is
64-bits we retrieve the upper and lower 32-bits into two separate registers. We
reform the original 64-bit value by using a shift and bitwise or
operation.Finally,  The two 64-bit values are subtracted to produce the final
elapsed time in cycles. We estimate the software overhead to be 15 cycles.

The total overhead of our measuring mechanism is estimated to be 17 cycles.

In order to gain the most accurate measure of the timing overhead we disabled
additional cores and frequency scaling within the bios.

For computing the overhead of reading time, we ran the timing harness that was
previously described without any intervening code.  Our results show that the
overhead of reading time to be 40 cycles. 

Our results show that we underestimated the overhead of our timing mechanism. We
believe this is due to our assumption that the instructions CPUID and RDTSCP,
which is a serializing version of RDTSC, executes in a single cycle. We were
able to verify this by removing the CPUID calls and replacing RDTSCP with RDTSC.
The experiment without the serializing instructions showed that the overhead of
the timing mechanism was 22 cycles which is closer to our estimated result.

\subsubsection{Measuring loop overhead}

For the looping overhead,  we have to be careful that our looping procedure does
not get optimized out either by the compiler or the assembly interpreter. For
example, an empty loop would be quickly optimized with a constant propagation
compiler pass.  In order to ensure that no optimizations are performed,  we
disable compiler optimizations by using the "-o0" flag and implemented our
procedure under test using inline assembly.

The procedure under test is a loop, written in 5 lines of assembly code, that
runs for 10 iterations.  We estimate that a loop would not add any hardware
overhead. In terms of software overhead, we estimate that 5 additional cycles
will be required for each iteration of the loop. This number was arrived at by
examining the assembly code that implements the loop and assuming that each
instruction executes in a single cycle.

Our results show that our estimate of the loop overhead was very close to the
measured loop overhead of 4 cycles. The discrepency is the result of counting an
instruction that initializes the number of loops to perform by moving a
parameter into a register. This instruction does not participate in the actual
loop.

An interesting result that we discovered is that after several iterations of the
timing procedure (i.e. timing and loop of ten iterations) the loop overhead is
reduced to a single cycle. We suspect this is due to optimizations which are
occuring in the hardware in spite of disabling optimizations during compile
time.

\subsection{Procedure call overhead}

\subsubsection{Estimation}

When a call instruction is invoked, the CPU has to do the following operations to
ensure that the call procedure execution happens smoothly : 1) push the returning
address and the call arguments on the call stack 2) move the instruction pointer to
the called function location. These operations require moving values around registers
but nothing else "fancy" needs to happen. Therefore, we can expect the overhead to
be in the order of tens of 10's nanoseconds.

\subsubsection{Methodology}

The methodology in this case is quite simple : we measure the time in nanoseconds
right before the procedure call, make the procedure call and measure the time again
as the first instruction of the procedure (then take the difference). However, there are some trickiness we need to take
into account. The procedure instructions initially live in memory, and we don't want to be
measuring memory access time to fetch those instructions. This is why we ignore the 
first measurement. After that, the instructions are expected to live in the cache. We do
not take into account time to load data from caches. In total, we do a 100 measurements
and take the average of all those measurements minus the first.

We created eight functions with zero to seven arguments. Each argument is \texttt{char} with one
a one byte length. The arguments themselves are not used
by the function. We do not know of compiler optimizations which remove unused argument from
functions, but if this is the case, then our implementation does not account for it. As a safety 
precaution, we added the \texttt{-O0} flag to our compiler to avoid optimization.

\subsubsection{Results}

Results are given in this table in nanoseconds. Note that unless otherwise specified, we
use the \texttt{clock\_gettime()} linux procedure to obtain time information with nanosecond
precision. It is considered as reliable by software industry [\ref{linux_time}].

\begin{center}
\begin{tabular}{| c | c | }
\hline 
Number of Arguments & Latency in ns \\
\hline
0 & 26 \\
1 & 26 \\
2 & 26 \\
3 & 26 \\
4 & 27 \\
5 & 27 \\
6 & 27 \\
7 & 28 \\
\hline
\end{tabular}
\end{center}

\subsubsection{Analysis}

We see that our estimation order of magnitude was correct. When it comes to the overhead of adding
an argument it seems minimal at best. However all our arguments are small (one byte long). 
Future work could go towards measuring overhead of larger arguments.

\subsection{System call overhead}

\subsubsection{Estimation}

System call are more involved than procedure calls. They also require setting the same information on the call stack,
but in addition, they require "trapping" to the kernel. When a system call occurs, the kernel is notified and a switch
to kernel mode. The switch requires 1) flushing processor registers 2) dispatching execution
to the proper kernel function 3) load the kernel state into the registers. Some systems calls have more complex requirements
but since we concentrate on a "minimal" system calls, we will consider only those for now. We expect an
overhead from 100's to 1000's of nanoseconds.

\subsection{Methodology}

The first problem that we encountered was to select an appropriate "minimal" system call. We decide to
go for a user-defined system call \texttt{SIG\_USR1}. \texttt{SIG\_USR1} traps to the kernel but returns control
to a (potentially different) user defined immediately afterwards. It is generally used to provide signal based
communication between processes. We however use it within a single process.

In our implementation, we first register a handler function with the signal as follows :

$$ \texttt{signal(SIGUSR1, signal\_handler);}$$

Immediately after making our first time measurement, we issue a kill command to
invoke the system call :

$$ \texttt{kill(get\_pid(), SIGUSR1);}$$

Again, the methodology in this case is quite simple : we measure the time in nanoseconds
right before the system call, make the system call and measure the time again
as the first instruction of the procedure (then take the difference). However, there are some trickiness we need to take
into account. The procedure instructions initially live in memory, and we don't want to be
measuring memory access time to fetch those instructions. This is why we ignore the 
first measurement. After that, the instructions are expected to live in the cache. We do
not take into account time to load data from caches. In total, we do a 100 measurements
and take the average of all those measurements minus the first.

\subsection{Task Creation Time}

In order to compare the execution time of a kernel thread and a process, we
setup a (slow) fibonacci function which would be the only thing the kernel
thread or process would execute.

We were able to profile kernel threads using our methodology. The procedure
between the two hardware counters simply creates a kernel thread which runs the
fibonacci function waits for that thread to complete. The average time spent by
the thread was 32264 cycles.

While kernel threads can be measured using our methodology, processes cannot be
created with a kernel module, since there is no fork or exec call in the linux
kernel API. As such for this task, we had to go out of kernel mode and write a
user program at the possible cost of loss of precision (however, this gave us
the opportunity to shut down compiler optimizations!). This time, we use a linux
\texttt{clone()} system call and had the child process execute the fibonacci
function. Measurements available soon.

\subsection{Context Switch Time} 

In order to measure the time taken for a context switch to occur, we implemented
a methodology using pipes to deterministically context switch between a parent
process or thread to its child and then back. Once a process is forked or a new
thread is cloned the parent reads from a pipe and the child writes a single
character of data to the pipe. The parent must block since there is no data
immediately available to be read, this causes a context switch to occur and the
child process to be scheduled for execution. The child writes to the pipe and
exits which causes another context switch back to the parent process which can
now read data from the pipe and then exit. 

We instantiate the pipes outside the timing harness in order to minimize the effect
of code that is not part of the context switch from affecting our timing
results. There are operations performed within our timing loop that will add
some overhead to our timing results (e.g. reads and writes to pipes), but we
believe the context switch time should dominate and the additional overhead will
not skew the usefulness of our results.

We perform the timing with the code that performs the context switch in loop
that performs 10,000 iterations. The elapsed time for a context switch is stored
on each iteration and the median of all results is output as the final result in
order to minimize the spurious results.

\subsubsection{Process Context Switch} 
We use the fork system call in order to create the child process which will
write to the pipe. We had a difficult time coming up with an estimate for the
time to perform a context switch. Since context switches in Linux are primarily
performed in software we believe the time should be proportional to the latency
in pushing all processor registers that must be saved to the kernel stack plus
the cost of installing a new address space for the process that is going to be
scheduled.

Our experimental results show that the median time to perform a process context
switch is 180,566 cycles which is approximately equal to 72 micro seconds on the
2.5GHz test machine. 

\subsubsection{Thread Context Switch}
We use the clone system call with the CLONE\_VM flag set in order to create a
thread which shares resources with the parent process. We faced the same
difficulty in estimating the time of context switching between threads, but we
can state with confidence that it should be at least an order of magnitude less
that the time taken to perform a context switch between processes because the
address space is shared.

Our experimental results show that the median time to perform a thread context
switch is 38,566 cycles which is approximately equal to 15 micro seconds on the
2.5GHz test machine.
