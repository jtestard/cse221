\subsection{Measurement Overhead}

\subsubsection{Estimation}

We estimate that the hardware overhead of reading the time-stamp registers and
calculating the elapsed time is 2 cycles. Since the time-stamp is implemented in
registers it would require 1 cycle to read the current time. 

We implemented the reading of the start and end time with inline assembly. The
code to read the start time and the code to read the end time each consists of
four assembly instructions. This includes the calls to CPUID which act as a
fence to force sequential execution of code.  Since the time-stamp register is
64-bits we retrieve the upper and lower 32-bits into two separate registers. We
reform the original 64-bit value by using a shift and bitwise or
operation.Finally,  The two 64-bit values are subtracted to produce the final
elapsed time in cycles. We estimate the software overhead to be 15 cycles.

The total overhead of our measuring mechanism is estimated to be 17 cycles.

\subsubsection{Methodology}

In order to accurately measure execution time we implemented a timing
methodology that reads the hardware time-stamp counter before and after a
procedure (by procedure here we mean lines of C or assembly code we wish to
profile, not necessarily a procedure call).  The time-stamp is read immediately
before the procedure is called and again after the procedure exits. The two
output values correspond to the start and end times of the procedure being
executed. The initial time is subtracted from the final time to calculate the
number of cycles that have elapsed.

The implementation for reading the time-stamp makes use of the assembly
instructions CPUID, RDTSC, and RDTSCP. CPUID and RDTSCP are used to force
serialization of instructions so that instructions which are not part of the
code being instrumented do not pollute our measurements due to out-of-order
execution.\cite{intel}

We run the timing harness and procedure being timed in a loop of 10,000
iterations. The execution time is collected on each iteration of the loop. We
use the median execution time over all iterations as the final result in order
to reduce the influence that extreme values caused by context switches,
interrupts and other exceptional events have on our experiment. 

In order to gain the most accurate measure of the timing overhead we disabled
additional cores and frequency scaling within the bios.

\subsubsection{Results} 

For computing the overhead of reading time, we ran the timing harness that was
previously described without any intervening code.  Our results show that the
overhead of reading time to be 40 cycles. We can convert cycles to elapsed time 
in seconds by the following equation:
$$\frac{40\text{cycles}}{2.45GHz} = 16.3ns$$

\subsubsection{Analysis}

Our results show that we underestimated the overhead of our timing mechanism. We
believe this is due to our assumption that the instructions CPUID and RDTSCP,
which is a serializing version of RDTSC, executes in a single cycle. We were
able to verify this by removing the CPUID calls and replacing RDTSCP with RDTSC.
The experiment without the serializing instructions showed that the overhead of
the timing mechanism was 22 cycles which is closer to our estimated result.

\subsection{Measuring loop overhead}

\subsubsection{Estimation}

We estimate that a loop would not add any hardware
overhead. In terms of software overhead, we estimate that 5 additional cycles
will be required for each iteration of the loop. This number was arrived at by
examining the assembly code that implements the loop and assuming that each
instruction executes in a single cycle.

\subsubsection{Methodology}

For the looping overhead,  we have to be careful that our looping procedure does
not get optimized out either by the compiler or the assembly interpreter. For
example, an empty loop would be quickly optimized with a constant propagation
compiler pass.  In order to ensure that no optimizations are performed,  we
disable compiler optimizations by using the "-o0" flag and implemented our
procedure under test using inline assembly.

The procedure under test is a loop, written in 5 lines of assembly code, that
runs for 10 iterations.

\subsubsection{Results}

The measured loop overhead was 4 cycles which is equal to 1.6ns.

\subsubsection{Analysis}

The discrepency is between our estimate and the result is due to the counting
an instruction that initializes the number of loops to perform . This
instruction does not participate in the actual loop.

An interesting result that we discovered is that after several iterations of the
timing procedure (i.e. timing and loop of ten iterations) the loop overhead is
reduced to a single cycle. We suspect this is due to optimizations which are
occuring in the hardware in spite of disabling optimizations during compile
time.

\subsection{Procedure call overhead}

\subsubsection{Estimation}

When a call instruction is invoked, the CPU has to do the following operations to
ensure that the call procedure execution happens smoothly : 1) push the returning
address and the call arguments on the call stack 2) move the instruction pointer to
the called function location. These operations require moving values around registers
but nothing else "fancy" needs to happen. Therefore, we can expect the overhead to
be in the order of tens of 10's nanoseconds.

\subsubsection{Methodology}

The methodology in this case is quite simple : we measure the time in nanoseconds
right before the procedure call, make the procedure call and measure the time again
as the first instruction of the procedure (then take the difference). However, there are some trickiness we need to take
into account. The procedure instructions initially live in memory, and we don't want to be
measuring memory access time to fetch those instructions. This is why we ignore the 
first measurement. After that, the instructions are expected to live in the cache. We do
not take into account time to load data from caches. In total, we do a 100 measurements
and take the average of all those measurements minus the first.

We created eight functions with zero to seven arguments. Each argument is \texttt{char} with one
a one byte length. The arguments themselves are not used
by the function. We do not know of compiler optimizations which remove unused argument from
functions, but if this is the case, then our implementation does not account for it. As a safety 
precaution, we added the \texttt{-O0} flag to our compiler to avoid optimization.

\subsubsection{Results}

Results are given in this table in nanoseconds. Note that unless otherwise specified, we
use the \texttt{clock\_gettime()} linux procedure to obtain time information with nanosecond
precision. It is considered as reliable by software industry [\ref{linux_time}].

\begin{center}
\begin{tabular}{| c | c | }
\hline 
Number of Arguments & Latency in ns \\
\hline
0 & 26 \\
1 & 26 \\
2 & 26 \\
3 & 26 \\
4 & 27 \\
5 & 27 \\
6 & 27 \\
7 & 28 \\
\hline
\end{tabular}
\end{center}

\subsubsection{Analysis}

We see that our estimation order of magnitude was correct. When it comes to the overhead of adding
an argument it seems minimal at best. However all our arguments are small (one byte long). 
Future work could go towards measuring overhead of larger arguments.

\subsection{System call overhead}

\subsubsection{Estimation}

System call are more involved than procedure calls. They also require setting the same information on the call stack,
but in addition, they require "trapping" to the kernel. When a system call occurs, the kernel is notified and a switch
to kernel mode. The switch requires 1) flushing processor registers 2) dispatching execution
to the proper kernel function 3) load the kernel state into the registers. Some systems calls have more complex requirements
but since we concentrate on a "minimal" system calls, we will consider only those for now. We expect an
overhead from high 100's to low 1000's of nanoseconds.

\subsubsection{Methodology}

The first problem that we encountered was to select an appropriate "minimal" system call. We decide to
go for a user-defined system call \texttt{SIG\_USR1}. \texttt{SIG\_USR1} traps to the kernel but returns control
to a (potentially different) user defined immediately afterwards. It is generally used to provide signal based
communication between processes. We however use it within a single process.

In our implementation, we first register a handler function with the signal as follows :

$$ \texttt{signal(SIGUSR1, signal\_handler);}$$

Immediately after making our first time measurement, we issue a kill command to
invoke the system call :

$$ \texttt{kill(get\_pid(), SIGUSR1);}$$

When notified, the kernel will transfer execution control back to the current process
but will move the instruction pointer to the beginning of the \texttt{signal\_handler} function.
Notice that we are here performing twice the process described in the estimation section
(going to kernel mode and back). Therefore, we will divide by two our timing results.

\subsubsection{Results}

After averaging over 100 runs, we obtained a timing an average of 5427 nanoseconds.
The last three significant digits across runs but the first stays fairly stable. We will thus
say that our system call has an overhead of about 2.5 microseconds (after dividing by 
two as described above). 

\subsubsection{Analysis of Results}

We had the luxury of being able to compare our results with those of LMBench [\ref{lmbench}].
The LMBench authors measured an overhead of 4 microseconds, but had machines that
were slower than ours. The results are slightly higher than our prediction but still reasonable.

\subsection{Kernel Thread and Process Creation Time}

Kernel thread and process creation were given in the same subsection in the project
specification, therefore we will consider them together.

\subsubsection{Estimation}

Process creation process is the most complicated process we have seen so far. Given we are
using Unix, note that we are measuring the overhead of \emph{forking} a process rather than
\emph{creating} it. We consider only forking from a single-threaded process. Forking requires 1) setting the thread table for the
process and setting the process descriptor and pid 3) allocating a new address space for the process
4) copying the state of the forking process to the forked one 5) creating a call stack and instruction pointer 
for the process thread. We expect the overall process to be quite expensive, in the order of hundreds of 
microseconds. Note though that some of the steps described above may be processed lazily, as we have 
seen in class (in particular copying the parent state over to the child). 

A Kernel thread (despite its name), is a lot more similar to a process than to a thread in the context of 
a multi-threaded program we have seen in class. In particular, Linux tends to consider threads as "just another process".
Yet the kernel thread creation time should still be a little shorter that the process forking time because :

\begin{itemize}
\item{Kernel Threads are created in kernel space, whereas forking is executed from user space through a system call.}
\item{Kernel Threads cannot have "threads of their own", removing the need for a thread table.}
\item{Kernel Threads do not share memory with their parent.}
\end{itemize} 

\subsection{Methodology}

Kernel thread creation and process creation profiling happens very differently. We evaluate process creation
within a user process in which we make a first time measurement before executing the fork system call, and 
have the first instruction within the child process be our second time measurement :

\begin{lstlisting}
clock_gettime(&ts_start);	
if ((pid = fork()) == 0) {
	clock_gettime(&ts_end);
	//...
\end{lstlisting}

Since state is copied across forked processes, the value of the time computed in the parent is available to the 
child. The kernel thread creation happens within a kernel module using the Linux \texttt{kthread\_create} kernel
function. We make our first time measurement right before the kernel function call. However, the state of the parent isn't copied with kernel threads. As a workaround we used the void pointer provided by the kernel function to pass the time measured.
Once in the kernel thread we measure the time again and take the difference.

\begin{lstlisting}
// In parent process
clock_gettime(&ts_start);	
void* data = &ts_start;
thread = kthread_create
   (thread_fn,data,...);
// In kernel thread
int thread_fn(void* data)
struct timespec ts_start =
   *((struct timespec*)data);
\end{lstlisting}

\subsubsection{Results}

In both cases, we did this experiment 100 times then took the average. We found an average creation time of  73.4 microseconds for kernel threads and 130 microseconds for forking a user process.

\subsubsection{Analysis of Results}

The results seem to match our expectations : when compared to regular processes, kernel threads are twice faster to create, but they are still very expensive. We also have the opportunity to compare our results with those of LMBench. We notice that we record a process creation time twice short than the fast found in the LMBench survey. We can find two explanations for this : 1) our hardware is faster than theirs 2) we record only the time to execute the fork, not the time to execute the exit.

\subsection{Context Switch Time} 

\subsubsection{Estimation}

\subsubsection{Methodology}

In order to measure the time taken for a context switch to occur, we implemented
a methodology using pipes to deterministically context switch between a parent
process or thread to its child and then back. Once a process is forked or a new
thread is cloned the parent reads from a pipe and the child writes a single
character of data to the pipe. The parent must block since there is no data
immediately available to be read, this causes a context switch to occur and the
child process to be scheduled for execution. The child writes to the pipe and
exits which causes another context switch back to the parent process which can
now read data from the pipe and then exit. 

We instantiate the pipes outside the timing harness in order to minimize the effect
of code that is not part of the context switch from affecting our timing
results. There are operations performed within our timing loop that will add
some overhead to our timing results (e.g. reads and writes to pipes), but we
believe the context switch time should dominate and the additional overhead will
not skew the usefulness of our results.

We perform the timing with the code that performs the context switch in loop
that performs 10,000 iterations. The elapsed time for a context switch is stored
on each iteration and the median of all results is output as the final result in
order to minimize the spurious results.

\subsubsection{Process Context Switch specific methodology} 
We use the fork system call in order to create the child process which will
write to the pipe. We had a difficult time coming up with an estimate for the
time to perform a context switch. Since context switches in Linux are primarily
performed in software we believe the time should be proportional to the latency
in pushing all processor registers that must be saved to the kernel stack plus
the cost of installing a new address space for the process that is going to be
scheduled.
\subsubsection{Thread Context Switch specific methodology}
We use the clone system call with the CLONE\_VM flag set in order to create a
thread which shares resources with the parent process. We faced the same
difficulty in estimating the time of context switching between threads, but we
can state with confidence that it should be at least an order of magnitude less
that the time taken to perform a context switch between processes because the
address space is shared.
\subsubsection{Result}

Our experimental results show that the median time to perform a process context
switch is 180,566 cycles which is approximately equal to 72 micro seconds on the
2.5GHz test machine. 


Our experimental results show that the median time to perform a thread context
switch is 38,566 cycles which is approximately equal to 15 micro seconds on the
2.5GHz test machine.

\subsubsection{Analysis}


